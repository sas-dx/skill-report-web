#!/usr/bin/env python3
"""
çµ±ä¸€è¨­è¨ˆãƒ„ãƒ¼ãƒ« - AIé§†å‹•ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†ææ©Ÿèƒ½

è¦æ±‚ä»•æ§˜ID: PLT.1-WEB.1
è¨­è¨ˆæ›¸: docs/design/architecture/æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯è¨­è¨ˆæ›¸.md
å®Ÿè£…æ—¥: 2025-07-08
å®Ÿè£…è€…: AIé§†å‹•é–‹ç™ºãƒãƒ¼ãƒ 
"""

import os
import sys
import time
import json
import hashlib
import threading
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
import logging

# å¤–éƒ¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ï¼‰
try:
    from watchdog.observers import Observer
    from watchdog.events import FileSystemEventHandler
    WATCHDOG_AVAILABLE = True
except ImportError:
    WATCHDOG_AVAILABLE = False
    print("Warning: watchdog not available. File monitoring disabled.")

try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False
    print("Warning: psutil not available. Performance monitoring limited.")

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class FileMetrics:
    """ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    path: str
    size: int
    lines: int
    modified_time: float
    hash_value: str
    file_type: str
    encoding: str = 'utf-8'

@dataclass
class QualityMetrics:
    """å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    file_path: str
    code_quality_score: float
    design_compliance_score: float
    requirement_id_coverage: float
    complexity_score: float
    maintainability_score: float
    test_coverage: float
    documentation_score: float
    timestamp: datetime

@dataclass
class PerformanceMetrics:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    operation: str
    duration: float
    memory_usage: float
    cpu_usage: float
    timestamp: datetime
    details: Dict[str, Any]

@dataclass
class AnalysisReport:
    """çµ±åˆåˆ†æãƒ¬ãƒãƒ¼ãƒˆ"""
    timestamp: datetime
    total_files: int
    quality_score: float
    performance_score: float
    compliance_score: float
    recommendations: List[str]
    trends: Dict[str, Any]
    alerts: List[str]

class FileChangeHandler:
    """ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´ç›£è¦–ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼ˆwatchdogéä¾å­˜ç‰ˆï¼‰"""
    
    def __init__(self, analytics_engine):
        self.analytics_engine = analytics_engine
        self.last_processed = {}
        self.debounce_time = 1.0  # 1ç§’ã®ãƒ‡ãƒã‚¦ãƒ³ã‚¹
    
    def check_file_changes(self, directory: str):
        """ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´ãƒã‚§ãƒƒã‚¯ï¼ˆãƒãƒ¼ãƒªãƒ³ã‚°æ–¹å¼ï¼‰"""
        for root, dirs, files in os.walk(directory):
            # é™¤å¤–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚¹ã‚­ãƒƒãƒ—
            dirs[:] = [d for d in dirs if d not in {'.git', 'node_modules', '__pycache__', '.next', 'venv'}]
            
            for file in files:
                file_path = os.path.join(root, file)
                if self._is_analysis_target(file_path):
                    try:
                        current_mtime = os.path.getmtime(file_path)
                        
                        if file_path not in self.last_processed or current_mtime > self.last_processed[file_path]:
                            self.last_processed[file_path] = current_mtime
                            logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´æ¤œçŸ¥: {file_path}")
                            self.analytics_engine.analyze_file_change(file_path)
                    
                    except OSError:
                        continue
    
    def _is_analysis_target(self, file_path: str) -> bool:
        """åˆ†æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã‹ãƒã‚§ãƒƒã‚¯"""
        target_extensions = {'.py', '.js', '.ts', '.tsx', '.md', '.yaml', '.yml', '.sql', '.json'}
        exclude_patterns = {'node_modules', '.git', '__pycache__', '.next', 'venv'}
        
        path_obj = Path(file_path)
        
        # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯
        for exclude in exclude_patterns:
            if exclude in path_obj.parts:
                return False
        
        # æ‹¡å¼µå­ãƒã‚§ãƒƒã‚¯
        return path_obj.suffix.lower() in target_extensions

class QualityAnalyzer:
    """å“è³ªåˆ†æã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self):
        self.requirement_patterns = [
            r'è¦æ±‚ä»•æ§˜ID:\s*([A-Z]{3}\.\d+-[A-Z]+\.\d+)',
            r'è¨­è¨ˆæ›¸:\s*([^\n]+)',
            r'å®Ÿè£…æ—¥:\s*(\d{4}-\d{2}-\d{2})',
        ]
    
    def analyze_file_quality(self, file_path: str) -> QualityMetrics:
        """ãƒ•ã‚¡ã‚¤ãƒ«å“è³ªåˆ†æ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # å„ç¨®ã‚¹ã‚³ã‚¢è¨ˆç®—
            code_quality = self._calculate_code_quality(content, file_path)
            design_compliance = self._calculate_design_compliance(content)
            requirement_coverage = self._calculate_requirement_coverage(content)
            complexity = self._calculate_complexity(content)
            maintainability = self._calculate_maintainability(content)
            test_coverage = self._calculate_test_coverage(file_path)
            documentation = self._calculate_documentation_score(content)
            
            return QualityMetrics(
                file_path=file_path,
                code_quality_score=code_quality,
                design_compliance_score=design_compliance,
                requirement_id_coverage=requirement_coverage,
                complexity_score=complexity,
                maintainability_score=maintainability,
                test_coverage=test_coverage,
                documentation_score=documentation,
                timestamp=datetime.now()
            )
        
        except Exception as e:
            logger.error(f"å“è³ªåˆ†æã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
            return QualityMetrics(
                file_path=file_path,
                code_quality_score=0.0,
                design_compliance_score=0.0,
                requirement_id_coverage=0.0,
                complexity_score=0.0,
                maintainability_score=0.0,
                test_coverage=0.0,
                documentation_score=0.0,
                timestamp=datetime.now()
            )
    
    def _calculate_code_quality(self, content: str, file_path: str) -> float:
        """ã‚³ãƒ¼ãƒ‰å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—"""
        score = 100.0
        
        # åŸºæœ¬çš„ãªå“è³ªãƒã‚§ãƒƒã‚¯
        lines = content.split('\n')
        
        # é•·ã™ãã‚‹è¡Œã®ãƒšãƒŠãƒ«ãƒ†ã‚£
        long_lines = sum(1 for line in lines if len(line) > 120)
        score -= min(long_lines * 2, 20)
        
        # ã‚³ãƒ¡ãƒ³ãƒˆç‡ãƒã‚§ãƒƒã‚¯
        comment_lines = sum(1 for line in lines if line.strip().startswith('#') or line.strip().startswith('//'))
        total_lines = len([line for line in lines if line.strip()])
        if total_lines > 0:
            comment_ratio = comment_lines / total_lines
            if comment_ratio < 0.1:
                score -= 15
            elif comment_ratio > 0.3:
                score -= 5
        
        # TODO/FIXME ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        todo_count = content.count('TODO') + content.count('FIXME')
        score -= min(todo_count * 3, 15)
        
        # é‡è¤‡ã‚³ãƒ¼ãƒ‰ã®ç°¡æ˜“ãƒã‚§ãƒƒã‚¯
        line_hashes = {}
        for line in lines:
            if len(line.strip()) > 10:
                line_hash = hashlib.md5(line.strip().encode()).hexdigest()
                line_hashes[line_hash] = line_hashes.get(line_hash, 0) + 1
        
        duplicate_lines = sum(count - 1 for count in line_hashes.values() if count > 1)
        score -= min(duplicate_lines * 1, 10)
        
        return max(score, 0.0)
    
    def _calculate_design_compliance(self, content: str) -> float:
        """è¨­è¨ˆæ›¸æº–æ‹ ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        score = 100.0
        
        # è¦æ±‚ä»•æ§˜ID ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        if 'è¦æ±‚ä»•æ§˜ID:' not in content:
            score -= 30
        
        # è¨­è¨ˆæ›¸å‚ç…§ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        if 'è¨­è¨ˆæ›¸:' not in content:
            score -= 20
        
        # å®Ÿè£…æ—¥ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        if 'å®Ÿè£…æ—¥:' not in content:
            score -= 10
        
        # ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯ï¼ˆ.mdãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
        if 'ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼' in content:
            score += 10  # ãƒœãƒ¼ãƒŠã‚¹ç‚¹
        elif content.endswith('.md'):
            score -= 25
        
        return max(score, 0.0)
    
    def _calculate_requirement_coverage(self, content: str) -> float:
        """è¦æ±‚ä»•æ§˜ID ã‚«ãƒãƒ¬ãƒƒã‚¸è¨ˆç®—"""
        import re
        
        # è¦æ±‚ä»•æ§˜ID ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
        pattern = r'[A-Z]{3}\.\d+-[A-Z]+\.\d+'
        matches = re.findall(pattern, content)
        
        if not matches:
            return 0.0
        
        # ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªè¦æ±‚ä»•æ§˜ID ã®æ•°
        unique_ids = set(matches)
        
        # åŸºæœ¬ã‚¹ã‚³ã‚¢ï¼ˆè¦æ±‚ä»•æ§˜ID ãŒå­˜åœ¨ã™ã‚Œã°80ç‚¹ï¼‰
        score = 80.0
        
        # è¤‡æ•°ã®è¦æ±‚ä»•æ§˜ID ãŒã‚ã‚Œã°è¿½åŠ ç‚¹
        if len(unique_ids) > 1:
            score += min(len(unique_ids) * 5, 20)
        
        return min(score, 100.0)
    
    def _calculate_complexity(self, content: str) -> float:
        """è¤‡é›‘åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆä½ã„ã»ã©è‰¯ã„ï¼‰"""
        lines = content.split('\n')
        
        # ã‚µã‚¤ã‚¯ãƒ­ãƒãƒ†ã‚£ãƒƒã‚¯è¤‡é›‘åº¦ã®ç°¡æ˜“è¨ˆç®—
        complexity_keywords = ['if', 'elif', 'else', 'for', 'while', 'try', 'except', 'case', 'switch']
        complexity_count = 0
        
        for line in lines:
            for keyword in complexity_keywords:
                complexity_count += line.count(keyword)
        
        # é–¢æ•°ãƒ»ãƒ¡ã‚½ãƒƒãƒ‰æ•°
        function_count = content.count('def ') + content.count('function ')
        
        if function_count == 0:
            return 100.0
        
        # å¹³å‡è¤‡é›‘åº¦
        avg_complexity = complexity_count / function_count if function_count > 0 else 0
        
        # ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆè¤‡é›‘åº¦ãŒä½ã„ã»ã©é«˜ã‚¹ã‚³ã‚¢ï¼‰
        if avg_complexity <= 5:
            return 100.0
        elif avg_complexity <= 10:
            return 80.0
        elif avg_complexity <= 15:
            return 60.0
        else:
            return max(40.0 - (avg_complexity - 15) * 2, 0.0)
    
    def _calculate_maintainability(self, content: str) -> float:
        """ä¿å®ˆæ€§ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        score = 100.0
        
        lines = content.split('\n')
        total_lines = len([line for line in lines if line.strip()])
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒšãƒŠãƒ«ãƒ†ã‚£
        if total_lines > 500:
            score -= min((total_lines - 500) * 0.1, 20)
        
        # é–¢æ•°ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
        in_function = False
        function_lines = 0
        max_function_lines = 0
        
        for line in lines:
            if 'def ' in line or 'function ' in line:
                if in_function:
                    max_function_lines = max(max_function_lines, function_lines)
                in_function = True
                function_lines = 0
            elif in_function and line.strip():
                function_lines += 1
        
        if max_function_lines > 50:
            score -= min((max_function_lines - 50) * 0.5, 15)
        
        # å‘½åè¦å‰‡ãƒã‚§ãƒƒã‚¯ï¼ˆç°¡æ˜“ï¼‰
        import re
        
        class_pattern = r'\bclass\s+([A-Z][a-zA-Z0-9]*)\b'
        class_matches = re.findall(class_pattern, content)
        if class_matches:
            # ã‚¯ãƒ©ã‚¹åãŒPascalCaseã‹ãƒã‚§ãƒƒã‚¯
            for class_name in class_matches:
                if not class_name[0].isupper():
                    score -= 5
        
        return max(score, 0.0)
    
    def _calculate_test_coverage(self, file_path: str) -> float:
        """ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸è¨ˆç®—ï¼ˆç°¡æ˜“ï¼‰"""
        # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        path_obj = Path(file_path)
        
        # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³
        test_patterns = [
            path_obj.parent / f"test_{path_obj.stem}.py",
            path_obj.parent / f"{path_obj.stem}_test.py",
            path_obj.parent / "tests" / f"test_{path_obj.stem}.py",
            path_obj.parent.parent / "tests" / f"test_{path_obj.stem}.py"
        ]
        
        for test_path in test_patterns:
            if test_path.exists():
                return 80.0
        
        # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«è‡ªä½“ã®å ´åˆ
        if 'test' in path_obj.name.lower():
            return 100.0
        
        return 0.0
    
    def _calculate_documentation_score(self, content: str) -> float:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¹ã‚³ã‚¢è¨ˆç®—"""
        score = 100.0
        
        # docstring ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        if '"""' not in content and "'''" not in content:
            score -= 30
        
        # ã‚³ãƒ¡ãƒ³ãƒˆå¯†åº¦ãƒã‚§ãƒƒã‚¯
        lines = content.split('\n')
        comment_lines = sum(1 for line in lines if line.strip().startswith('#'))
        code_lines = sum(1 for line in lines if line.strip() and not line.strip().startswith('#'))
        
        if code_lines > 0:
            comment_ratio = comment_lines / code_lines
            if comment_ratio < 0.1:
                score -= 20
        
        return max(score, 0.0)

class PerformanceMonitor:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–"""
    
    def __init__(self):
        self.metrics_history = []
        self.start_time = time.time()
        if PSUTIL_AVAILABLE:
            self.process = psutil.Process()
        else:
            self.process = None
    
    def start_operation(self, operation: str) -> Dict[str, Any]:
        """æ“ä½œé–‹å§‹"""
        context = {
            'operation': operation,
            'start_time': time.time(),
        }
        
        if self.process:
            context['start_memory'] = self.process.memory_info().rss
            context['start_cpu'] = self.process.cpu_percent()
        else:
            context['start_memory'] = 0
            context['start_cpu'] = 0
        
        return context
    
    def end_operation(self, context: Dict[str, Any], details: Dict[str, Any] = None) -> PerformanceMetrics:
        """æ“ä½œçµ‚äº†"""
        end_time = time.time()
        duration = end_time - context['start_time']
        
        if self.process:
            end_memory = self.process.memory_info().rss
            end_cpu = self.process.cpu_percent()
            memory_usage = end_memory - context['start_memory']
            cpu_usage = end_cpu
        else:
            memory_usage = 0
            cpu_usage = 0
        
        metrics = PerformanceMetrics(
            operation=context['operation'],
            duration=duration,
            memory_usage=memory_usage,
            cpu_usage=cpu_usage,
            timestamp=datetime.now(),
            details=details or {}
        )
        
        self.metrics_history.append(metrics)
        return metrics
    
    def get_system_metrics(self) -> Dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—"""
        if PSUTIL_AVAILABLE and self.process:
            return {
                'cpu_percent': psutil.cpu_percent(interval=1),
                'memory_percent': psutil.virtual_memory().percent,
                'disk_usage': psutil.disk_usage('/').percent,
                'process_memory': self.process.memory_info().rss,
                'process_cpu': self.process.cpu_percent(),
                'uptime': time.time() - self.start_time
            }
        else:
            return {
                'cpu_percent': 0,
                'memory_percent': 0,
                'disk_usage': 0,
                'process_memory': 0,
                'process_cpu': 0,
                'uptime': time.time() - self.start_time
            }

class RealtimeAnalyticsEngine:
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, watch_directories: List[str] = None):
        self.watch_directories = watch_directories or ['.']
        self.quality_analyzer = QualityAnalyzer()
        self.performance_monitor = PerformanceMonitor()
        self.file_metrics = {}
        self.quality_metrics = {}
        self.analysis_history = []
        self.running = False
        
        # åˆ†æçµæœä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.output_dir = Path('docs/tools/reports/analytics')
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
        self.file_handler = FileChangeHandler(self)
    
    def start_monitoring(self):
        """ç›£è¦–é–‹å§‹"""
        logger.info("ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æç›£è¦–ã‚’é–‹å§‹ã—ã¾ã™")
        self.running = True
        
        # åˆæœŸåˆ†æå®Ÿè¡Œ
        self._perform_initial_analysis()
        
        # ç¶™ç¶šç›£è¦–ï¼ˆãƒãƒ¼ãƒªãƒ³ã‚°æ–¹å¼ï¼‰
        if WATCHDOG_AVAILABLE:
            logger.info("watchdogä½¿ç”¨: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–")
            # watchdogå®Ÿè£…ã¯çœç•¥ï¼ˆä¾å­˜é–¢ä¿‚ã‚’æ¸›ã‚‰ã™ãŸã‚ï¼‰
        else:
            logger.info("ãƒãƒ¼ãƒªãƒ³ã‚°æ–¹å¼: å®šæœŸç›£è¦–")
            self._start_polling_monitor()
    
    def stop_monitoring(self):
        """ç›£è¦–åœæ­¢"""
        self.running = False
        logger.info("ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æç›£è¦–ã‚’åœæ­¢ã—ã¾ã—ãŸ")
    
    def _start_polling_monitor(self):
        """ãƒãƒ¼ãƒªãƒ³ã‚°ç›£è¦–é–‹å§‹"""
        def monitor_loop():
            while self.running:
                for directory in self.watch_directories:
                    if os.path.exists(directory):
                        self.file_handler.check_file_changes(directory)
                time.sleep(5)  # 5ç§’é–“éš”ã§ãƒã‚§ãƒƒã‚¯
        
        monitor_thread = threading.Thread(target=monitor_loop, daemon=True)
        monitor_thread.start()
    
    def analyze_file_change(self, file_path: str):
        """ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´åˆ†æ"""
        context = self.performance_monitor.start_operation(f"analyze_file_change:{file_path}")
        
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
            file_metrics = self._get_file_metrics(file_path)
            self.file_metrics[file_path] = file_metrics
            
            # å“è³ªåˆ†æå®Ÿè¡Œ
            quality_metrics = self.quality_analyzer.analyze_file_quality(file_path)
            self.quality_metrics[file_path] = quality_metrics
            
            # å½±éŸ¿ç¯„å›²åˆ†æ
            impact_analysis = self._analyze_impact(file_path)
            
            # åˆ†æçµæœä¿å­˜
            self._save_analysis_result(file_path, quality_metrics, impact_analysis)
            
            logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æå®Œäº†: {file_path} (å“è³ªã‚¹ã‚³ã‚¢: {quality_metrics.code_quality_score:.1f})")
            
        except Exception as e:
            logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
        
        finally:
            self.performance_monitor.end_operation(context, {'file_path': file_path})
    
    def generate_analysis_report(self) -> AnalysisReport:
        """çµ±åˆåˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        context = self.performance_monitor.start_operation("generate_analysis_report")
        
        try:
            total_files = len(self.quality_metrics)
            
            if total_files == 0:
                return AnalysisReport(
                    timestamp=datetime.now(),
                    total_files=0,
                    quality_score=0.0,
                    performance_score=0.0,
                    compliance_score=0.0,
                    recommendations=[],
                    trends={},
                    alerts=[]
                )
            
            # å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—
            quality_scores = [m.code_quality_score for m in self.quality_metrics.values()]
            avg_quality = sum(quality_scores) / len(quality_scores)
            
            # ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹ã‚¹ã‚³ã‚¢è¨ˆç®—
            compliance_scores = [m.design_compliance_score for m in self.quality_metrics.values()]
            avg_compliance = sum(compliance_scores) / len(compliance_scores)
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¹ã‚³ã‚¢è¨ˆç®—
            recent_metrics = self.performance_monitor.metrics_history[-10:]
            if recent_metrics:
                avg_duration = sum(m.duration for m in recent_metrics) / len(recent_metrics)
                performance_score = max(100 - avg_duration * 10, 0)
            else:
                performance_score = 100.0
            
            # æ¨å¥¨äº‹é …ç”Ÿæˆ
            recommendations = self._generate_recommendations()
            
            # ã‚¢ãƒ©ãƒ¼ãƒˆç”Ÿæˆ
            alerts = self._generate_alerts()
            
            # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
            trends = self._analyze_trends()
            
            report = AnalysisReport(
                timestamp=datetime.now(),
                total_files=total_files,
                quality_score=avg_quality,
                performance_score=performance_score,
                compliance_score=avg_compliance,
                recommendations=recommendations,
                trends=trends,
                alerts=alerts
            )
            
            # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
            self._save_report(report)
            
            return report
        
        finally:
            self.performance_monitor.end_operation(context)
    
    def _perform_initial_analysis(self):
        """åˆæœŸåˆ†æå®Ÿè¡Œ"""
        logger.info("åˆæœŸåˆ†æã‚’å®Ÿè¡Œä¸­...")
        
        for directory in self.watch_directories:
            for root, dirs, files in os.walk(directory):
                # é™¤å¤–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚¹ã‚­ãƒƒãƒ—
                dirs[:] = [d for d in dirs if d not in {'.git', 'node_modules', '__pycache__', '.next', 'venv'}]
                
                for file in files:
                    file_path = os.path.join(root, file)
                    if self.file_handler._is_analysis_target(file_path):
                        self.analyze_file_change(file_path)
        
        logger.info("åˆæœŸåˆ†æå®Œäº†")
    
    def _get_file_metrics(self, file_path: str) -> FileMetrics:
        """ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—"""
        try:
            stat = os.stat(file_path)
            
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            lines = len(content.split('\n'))
            hash_value = hashlib.md5(content.encode()).hexdigest()
            file_type = Path(file_path).suffix
            
            return FileMetrics(
                path=file_path,
                size=stat.st_size,
                lines=lines,
                modified_time=stat.st_mtime,
                hash_value=hash_value,
                file_type=file_type
            )
        
        except Exception as e:
            logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—ã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
            return FileMetrics(
                path=file_path,
                size=0,
                lines=0,
                modified_time=0,
                hash_value='',
                file_type=''
            )
    
    def _analyze_impact(self, file_path: str) -> Dict[str, Any]:
        """å½±éŸ¿ç¯„å›²åˆ†æ"""
        impact = {
            'modified_file': file_path,
            'related_files': [],
            'risk_level': 'low',
            'estimated_impact': []
        }
        
        # é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢ï¼ˆç°¡æ˜“å®Ÿè£…ï¼‰
        file_name = Path(file_path).stem
        
        for other_path in self.file_metrics.keys():
            if other_path != file_path:
                try:
                    with open(other_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    if file_name in content:
                        impact['related_files'].append(other_path)
                
                except Exception:
                    continue
        
        # ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«åˆ¤å®š
        if len(impact['related_files']) > 5:
            impact['risk_level'] = 'high'
        elif len(impact['related_files']) > 2:
            impact['risk_level'] = 'medium'
        
        return impact
    
    def _generate_recommendations(self) -> List[str]:
        """æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = []
        
        # å“è³ªã‚¹ã‚³ã‚¢ãŒä½ã„ãƒ•ã‚¡ã‚¤ãƒ«ã®ç‰¹å®š
        low_quality_files = [
            path for path, metrics in self.quality_metrics.items()
            if metrics.code_quality_score < 70
        ]
        
        if low_quality_files:
            recommendations.append(f"å“è³ªæ”¹å–„ãŒå¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«: {len(low_quality_files)}ä»¶")
        
        # è¦æ±‚ä»•æ§˜ID æœªå¯¾å¿œãƒ•ã‚¡ã‚¤ãƒ«ã®ç‰¹å®š
        missing_req_files = [
            path for path, metrics in self.quality_metrics.items()
            if metrics.requirement_id_coverage < 50
        ]
        
        if missing_req_files:
            recommendations.append(f"è¦æ±‚ä»•æ§˜IDå¯¾å¿œãŒå¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«: {len(missing_req_files)}ä»¶")
        
        # ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸æ”¹å–„
        untested_files = [
            path for path, metrics in self.quality_metrics.items()
            if metrics.test_coverage < 50
        ]
        
        if untested_files:
            recommendations.append(f"ãƒ†ã‚¹ãƒˆå®Ÿè£…ãŒå¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«: {len(untested_files)}ä»¶")
        
        return recommendations
    
    def _generate_alerts(self) -> List[str]:
        """ã‚¢ãƒ©ãƒ¼ãƒˆç”Ÿæˆ"""
        alerts = []
        
        # é‡å¤§ãªå“è³ªå•é¡Œ
        critical_files = [
            path for path, metrics in self.quality_metrics.items()
            if metrics.code_quality_score < 50
        ]
        
        if critical_files:
            alerts.append(f"ğŸš¨ é‡å¤§ãªå“è³ªå•é¡Œ: {len(critical_files)}ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§å“è³ªã‚¹ã‚³ã‚¢50æœªæº€")
        
        # è¨­è¨ˆæ›¸éæº–æ‹ 
        non_compliant_files = [
            path for path, metrics in self.quality_metrics.items()
            if metrics.design_compliance_score < 60
        ]
        
        if non_compliant_files:
            alerts.append(f"âš ï¸ è¨­è¨ˆæ›¸éæº–æ‹ : {len(non_compliant_files)}ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«")
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å•é¡Œ
        recent_metrics = self.performance_monitor.metrics_history[-5:]
        if recent_metrics:
            avg_duration = sum(m.duration for m in recent_metrics) / len(recent_metrics)
            if avg_duration > 5.0:
                alerts.append(f"ğŸŒ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ä½ä¸‹: å¹³å‡å‡¦ç†æ™‚é–“ {avg_duration:.2f}ç§’")
        
        return alerts
    
    def _analyze_trends(self) -> Dict[str, Any]:
        """ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ"""
        trends = {
            'quality_trend': 'stable',
            'file_count_trend': 'stable',
            'performance_trend': 'stable'
        }
        
        # å“è³ªãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æï¼ˆç°¡æ˜“ï¼‰
        if len(self.analysis_history) > 1:
            recent_quality = self.analysis_history[-1].quality_score
            previous_quality = self.analysis_history[-2].quality_score
            
            if recent_quality > previous_quality + 5:
                trends['quality_trend'] = 'improving'
            elif recent_quality < previous_quality - 5:
                trends['quality_trend'] = 'declining'
        
        return trends
    
    def _save_analysis_result(self, file_path: str, quality_metrics: QualityMetrics, impact_analysis: Dict[str, Any]):
        """åˆ†æçµæœä¿å­˜"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        result_file = self.output_dir / f"analysis_{timestamp}_{Path(file_path).stem}.json"
        
        result = {
            'file_path': file_path,
            'timestamp': timestamp,
            'quality_metrics': asdict(quality_metrics),
            'impact_analysis': impact_analysis
        }
        
        try:
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, indent=2, ensure_ascii=False, default=str)
        except Exception as e:
            logger.error(f"åˆ†æçµæœä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _save_report(self, report: AnalysisReport):
        """ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_file = self.output_dir / f"report_{timestamp}.json"
        
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(asdict(report), f, indent=2, ensure_ascii=False, default=str)
            
            # æœ€æ–°ãƒ¬ãƒãƒ¼ãƒˆã¨ã—ã¦ã‚‚ã‚³ãƒ”ãƒ¼
            latest_file = self.output_dir / "latest_report.json"
            with open(latest_file, 'w', encoding='utf-8') as f:
                json.dump(asdict(report), f, indent=2, ensure_ascii=False, default=str)
            
            logger.info(f"åˆ†æãƒ¬ãƒãƒ¼ãƒˆä¿å­˜å®Œäº†: {report_file}")
            
        except Exception as e:
            logger.error(f"ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='çµ±ä¸€è¨­è¨ˆãƒ„ãƒ¼ãƒ« - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æ')
    parser.add_argument('--watch', nargs='+', default=['.'], help='ç›£è¦–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    parser.add_argument('--report-only', action='store_true', help='ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã®ã¿å®Ÿè¡Œ')
    parser.add_argument('--output', default='docs/tools/reports/analytics', help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    parser.add_argument('--continuous', action='store_true', help='ç¶™ç¶šç›£è¦–ãƒ¢ãƒ¼ãƒ‰')
    
    args = parser.parse_args()
    
    # åˆ†æã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
    engine = RealtimeAnalyticsEngine(watch_directories=args.watch)
    
    if args.report_only:
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã®ã¿
        logger.info("ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œä¸­...")
        report = engine.generate_analysis_report()
        
        print("\n" + "="*60)
        print("ğŸ“Š çµ±ä¸€è¨­è¨ˆãƒ„ãƒ¼ãƒ« - åˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
        print("="*60)
        print(f"ğŸ“… å®Ÿè¡Œæ—¥æ™‚: {report.timestamp}")
        print(f"ğŸ“ å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {report.total_files}")
        print(f"â­ å“è³ªã‚¹ã‚³ã‚¢: {report.quality_score:.1f}/100")
        print(f"ğŸš€ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¹ã‚³ã‚¢: {report.performance_score:.1f}/100")
        print(f"ğŸ“‹ è¨­è¨ˆæ›¸æº–æ‹ ã‚¹ã‚³ã‚¢: {report.compliance_score:.1f}/100")
        
        if report.recommendations:
            print("\nğŸ“ æ¨å¥¨äº‹é …:")
            for rec in report.recommendations:
                print(f"  â€¢ {rec}")
        
        if report.alerts:
            print("\nğŸš¨ ã‚¢ãƒ©ãƒ¼ãƒˆ:")
            for alert in report.alerts:
                print(f"  â€¢ {alert}")
        
        print(f"\nğŸ“„ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ: {engine.output_dir}/latest_report.json")
        print("="*60)
        
    elif args.continuous:
        # ç¶™ç¶šç›£è¦–ãƒ¢ãƒ¼ãƒ‰
        logger.info("ç¶™ç¶šç›£è¦–ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œä¸­...")
        try:
            engine.start_monitoring()
            print("ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æã‚’é–‹å§‹ã—ã¾ã—ãŸã€‚Ctrl+Cã§åœæ­¢ã—ã¾ã™ã€‚")
            
            while True:
                time.sleep(30)  # 30ç§’ã”ã¨ã«ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
                report = engine.generate_analysis_report()
                print(f"[{datetime.now().strftime('%H:%M:%S')}] å“è³ªã‚¹ã‚³ã‚¢: {report.quality_score:.1f}, ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {report.total_files}")
                
        except KeyboardInterrupt:
            print("\nç›£è¦–ã‚’åœæ­¢ã—ã¦ã„ã¾ã™...")
            engine.stop_monitoring()
            print("ç›£è¦–ã‚’åœæ­¢ã—ã¾ã—ãŸã€‚")
    
    else:
        # ä¸€å›é™ã‚Šã®åˆ†æ
        logger.info("ä¸€å›é™ã‚Šã®åˆ†æã‚’å®Ÿè¡Œä¸­...")
        engine._perform_initial_analysis()
        report = engine.generate_analysis_report()
        
        print(f"\nåˆ†æå®Œäº†: {report.total_files}ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æ")
        print(f"å“è³ªã‚¹ã‚³ã‚¢: {report.quality_score:.1f}/100")
        print(f"è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ: {engine.output_dir}/latest_report.json")

if __name__ == "__main__":
    main()
