#!/usr/bin/env python3
"""
ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆãƒ„ãƒ¼ãƒ«çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆå®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ

è¦æ±‚ä»•æ§˜ID: PLT.1-WEB.1, SKL.1-HIER.1
è¨­è¨ˆæ›¸: docs/design/database/08-database-design-guidelines.md
å®Ÿè£…æ—¥: 2025-06-21
å®Ÿè£…è€…: AI Assistant

å…¨ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã®çµ±åˆå®Ÿè¡Œï¼š
- ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ
- çµ±åˆãƒ†ã‚¹ãƒˆ
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
- ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
"""

import sys
import unittest
import time
from pathlib import Path
from typing import List, Dict, Any
import json

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# ãƒ†ã‚¹ãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from tests.unit.consistency_checker.test_consistency_checks import *
    from tests.unit.shared.test_shared_components import *
    from tests.integration.test_tool_integration import *
    from tests.performance.test_performance import *
except ImportError as e:
    print(f"è­¦å‘Š: ä¸€éƒ¨ã®ãƒ†ã‚¹ãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã§ãã¾ã›ã‚“ã§ã—ãŸ: {e}")


class TestSuiteRunner:
    """ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆå®Ÿè¡Œç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.results = {}
        self.start_time = None
        self.end_time = None
    
    def run_all_tests(self) -> Dict[str, Any]:
        """å…¨ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã‚’å®Ÿè¡Œ"""
        print("=" * 80)
        print("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆãƒ„ãƒ¼ãƒ«çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆå®Ÿè¡Œé–‹å§‹")
        print("=" * 80)
        
        self.start_time = time.time()
        
        # ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆå®šç¾©
        test_suites = [
            ('ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ - æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯', self._run_consistency_checker_tests),
            ('ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ - å…±æœ‰ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ', self._run_shared_components_tests),
            ('çµ±åˆãƒ†ã‚¹ãƒˆ', self._run_integration_tests),
            ('ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ', self._run_performance_tests)
        ]
        
        # å„ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã‚’å®Ÿè¡Œ
        for suite_name, test_func in test_suites:
            print(f"\n{'-' * 60}")
            print(f"å®Ÿè¡Œä¸­: {suite_name}")
            print(f"{'-' * 60}")
            
            try:
                result = test_func()
                self.results[suite_name] = result
                print(f"âœ… {suite_name}: å®Œäº†")
            except Exception as e:
                print(f"âŒ {suite_name}: ã‚¨ãƒ©ãƒ¼ - {str(e)}")
                self.results[suite_name] = {
                    'status': 'error',
                    'error': str(e),
                    'tests_run': 0,
                    'failures': 1,
                    'errors': 1
                }
        
        self.end_time = time.time()
        
        # çµæžœã‚µãƒžãƒªãƒ¼å‡ºåŠ›
        self._print_summary()
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        self._generate_report()
        
        return self.results
    
    def _run_consistency_checker_tests(self) -> Dict[str, Any]:
        """æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        loader = unittest.TestLoader()
        suite = unittest.TestSuite()
        
        # ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹ã‚’è¿½åŠ 
        test_classes = [
            'TestConsistencyChecker',
            'TestTableExistenceChecker',
            'TestColumnConsistencyChecker',
            'TestForeignKeyChecker',
            'TestNamingConventionChecker',
            'TestOrphanedFileChecker'
        ]
        
        for class_name in test_classes:
            try:
                test_class = globals().get(class_name)
                if test_class:
                    suite.addTests(loader.loadTestsFromTestCase(test_class))
            except Exception as e:
                print(f"è­¦å‘Š: {class_name} ã‚’ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã›ã‚“ã§ã—ãŸ: {e}")
        
        # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        runner = unittest.TextTestRunner(verbosity=2, stream=sys.stdout)
        result = runner.run(suite)
        
        return {
            'status': 'success' if result.wasSuccessful() else 'failure',
            'tests_run': result.testsRun,
            'failures': len(result.failures),
            'errors': len(result.errors),
            'skipped': len(result.skipped) if hasattr(result, 'skipped') else 0
        }
    
    def _run_shared_components_tests(self) -> Dict[str, Any]:
        """å…±æœ‰ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        loader = unittest.TestLoader()
        suite = unittest.TestSuite()
        
        # ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹ã‚’è¿½åŠ 
        test_classes = [
            'TestDatabaseToolsConfig',
            'TestLogger',
            'TestYAMLParserAdvanced',
            'TestDDLParserAdvanced',
            'TestFilesystemAdapter',
            'TestDataTransformAdapter',
            'TestFileManager',
            'TestErrorHandlingIntegration'
        ]
        
        for class_name in test_classes:
            try:
                test_class = globals().get(class_name)
                if test_class:
                    suite.addTests(loader.loadTestsFromTestCase(test_class))
            except Exception as e:
                print(f"è­¦å‘Š: {class_name} ã‚’ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã›ã‚“ã§ã—ãŸ: {e}")
        
        # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        runner = unittest.TextTestRunner(verbosity=2, stream=sys.stdout)
        result = runner.run(suite)
        
        return {
            'status': 'success' if result.wasSuccessful() else 'failure',
            'tests_run': result.testsRun,
            'failures': len(result.failures),
            'errors': len(result.errors),
            'skipped': len(result.skipped) if hasattr(result, 'skipped') else 0
        }
    
    def _run_integration_tests(self) -> Dict[str, Any]:
        """çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        loader = unittest.TestLoader()
        suite = unittest.TestSuite()
        
        # çµ±åˆãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹ã‚’è¿½åŠ 
        test_classes = [
            'TestToolIntegration',
            'TestEndToEndWorkflow',
            'TestCrossToolConsistency'
        ]
        
        for class_name in test_classes:
            try:
                test_class = globals().get(class_name)
                if test_class:
                    suite.addTests(loader.loadTestsFromTestCase(test_class))
            except Exception as e:
                print(f"è­¦å‘Š: {class_name} ã‚’ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã›ã‚“ã§ã—ãŸ: {e}")
        
        # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        runner = unittest.TextTestRunner(verbosity=2, stream=sys.stdout)
        result = runner.run(suite)
        
        return {
            'status': 'success' if result.wasSuccessful() else 'failure',
            'tests_run': result.testsRun,
            'failures': len(result.failures),
            'errors': len(result.errors),
            'skipped': len(result.skipped) if hasattr(result, 'skipped') else 0
        }
    
    def _run_performance_tests(self) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        try:
            import psutil
            psutil_available = True
        except ImportError:
            psutil_available = False
            print("è­¦å‘Š: psutil ãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        
        if not psutil_available:
            return {
                'status': 'skipped',
                'tests_run': 0,
                'failures': 0,
                'errors': 0,
                'skipped': 1,
                'reason': 'psutil not available'
            }
        
        loader = unittest.TestLoader()
        suite = unittest.TestSuite()
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹ã‚’è¿½åŠ 
        test_classes = [
            'TestTableGenerationPerformance',
            'TestConsistencyCheckPerformance'
        ]
        
        for class_name in test_classes:
            try:
                test_class = globals().get(class_name)
                if test_class:
                    suite.addTests(loader.loadTestsFromTestCase(test_class))
            except Exception as e:
                print(f"è­¦å‘Š: {class_name} ã‚’ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã›ã‚“ã§ã—ãŸ: {e}")
        
        # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        runner = unittest.TextTestRunner(verbosity=2, stream=sys.stdout)
        result = runner.run(suite)
        
        return {
            'status': 'success' if result.wasSuccessful() else 'failure',
            'tests_run': result.testsRun,
            'failures': len(result.failures),
            'errors': len(result.errors),
            'skipped': len(result.skipped) if hasattr(result, 'skipped') else 0
        }
    
    def _print_summary(self):
        """ãƒ†ã‚¹ãƒˆçµæžœã‚µãƒžãƒªãƒ¼å‡ºåŠ›"""
        print("\n" + "=" * 80)
        print("ãƒ†ã‚¹ãƒˆå®Ÿè¡Œçµæžœã‚µãƒžãƒªãƒ¼")
        print("=" * 80)
        
        total_tests = 0
        total_failures = 0
        total_errors = 0
        total_skipped = 0
        
        for suite_name, result in self.results.items():
            status_icon = "âœ…" if result['status'] == 'success' else "âŒ" if result['status'] == 'error' else "â­ï¸"
            print(f"{status_icon} {suite_name}:")
            print(f"    å®Ÿè¡Œ: {result['tests_run']}, å¤±æ•—: {result['failures']}, ã‚¨ãƒ©ãƒ¼: {result['errors']}, ã‚¹ã‚­ãƒƒãƒ—: {result.get('skipped', 0)}")
            
            total_tests += result['tests_run']
            total_failures += result['failures']
            total_errors += result['errors']
            total_skipped += result.get('skipped', 0)
        
        print(f"\nðŸ“Š ç·åˆçµæžœ:")
        print(f"    ç·ãƒ†ã‚¹ãƒˆæ•°: {total_tests}")
        print(f"    æˆåŠŸ: {total_tests - total_failures - total_errors}")
        print(f"    å¤±æ•—: {total_failures}")
        print(f"    ã‚¨ãƒ©ãƒ¼: {total_errors}")
        print(f"    ã‚¹ã‚­ãƒƒãƒ—: {total_skipped}")
        
        execution_time = self.end_time - self.start_time
        print(f"    å®Ÿè¡Œæ™‚é–“: {execution_time:.2f}ç§’")
        
        # ç·åˆåˆ¤å®š
        if total_failures == 0 and total_errors == 0:
            print(f"\nðŸŽ‰ å…¨ãƒ†ã‚¹ãƒˆæˆåŠŸï¼")
        else:
            print(f"\nâš ï¸  ä¸€éƒ¨ãƒ†ã‚¹ãƒˆã«å•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚è©³ç´°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    
    def _generate_report(self):
        """ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        report_data = {
            'test_execution': {
                'start_time': self.start_time,
                'end_time': self.end_time,
                'duration': self.end_time - self.start_time,
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())
            },
            'results': self.results,
            'summary': self._calculate_summary()
        }
        
        # JSONãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report_file = project_root / 'test_execution_report.json'
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nðŸ“„ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {report_file}")
    
    def _calculate_summary(self) -> Dict[str, Any]:
        """ã‚µãƒžãƒªãƒ¼è¨ˆç®—"""
        total_tests = sum(r['tests_run'] for r in self.results.values())
        total_failures = sum(r['failures'] for r in self.results.values())
        total_errors = sum(r['errors'] for r in self.results.values())
        total_skipped = sum(r.get('skipped', 0) for r in self.results.values())
        
        success_rate = ((total_tests - total_failures - total_errors) / total_tests * 100) if total_tests > 0 else 0
        
        return {
            'total_tests': total_tests,
            'total_failures': total_failures,
            'total_errors': total_errors,
            'total_skipped': total_skipped,
            'success_rate': round(success_rate, 2),
            'overall_status': 'success' if total_failures == 0 and total_errors == 0 else 'failure'
        }


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    runner = TestSuiteRunner()
    results = runner.run_all_tests()
    
    # çµ‚äº†ã‚³ãƒ¼ãƒ‰è¨­å®š
    summary = runner._calculate_summary()
    exit_code = 0 if summary['overall_status'] == 'success' else 1
    
    print(f"\nçµ‚äº†ã‚³ãƒ¼ãƒ‰: {exit_code}")
    sys.exit(exit_code)


if __name__ == '__main__':
    main()
